name: Scrape Website

on:
  schedule:
    - cron: '0 0 * * *' # This cron job runs daily at midnight UTC
  workflow_dispatch: # Allows manual triggering of the workflow

permissions:
  contents: write  # Grant write permissions to the repository contents
  issues: write    # Grant write permissions to create issues

jobs:
  scrape_job:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout repository
      uses: actions/checkout@v3

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.x'  # Use the version of Python you need

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install requests beautifulsoup4

    - name: Run scraper
      run: python scraper.py

    - name: Move HTML and CSV files to docs folder
      run: |
        mv scraped_table_en.html docs/scraped_table_en.html
        mv scraped_table_fr.html docs/scraped_table_fr.html
        mv scraped_content_eng.html docs/scraped_content_eng.html
        mv scraped_content_fra.html docs/scraped_content_fra.html

    - name: Commit and Push Result
      id: git_commit
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}  # Use the automatically provided GITHUB_TOKEN
      run: |
        git config --global user.email "you@example.com"
        git config --global user.name "Your Name"
        git add docs/scraped_table_en.html docs/scraped_table_fr.html docs/scraped_content_eng.html docs/scraped_content_fra.html
        git commit --allow-empty -m "Update scraped data and content"
        git push
      continue-on-error: true

    - name: Check for Changes
      id: check_changes
      run: |
        COMMIT_MESSAGE=$(
